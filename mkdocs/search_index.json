{
    "docs": [
        {
            "location": "/", 
            "text": "About\n\n\nAffectiveTweets\n is a \nWEKA\n package for analyzing emotion and sentiment of English written tweets.  The source code is hosted on \nGithub\n.\n\n\nThe package implements WEKA filters for calculating state-of-the-art affective analysis features from tweets that can be fed into machine learning algorithms. Many of these features were drawn from the \nNRC-Canada System\n. It also implements methods for building affective lexicons and distant supervision methods for training affective models from unlabelled tweets.\n\n\nThe package was also made available as the official baseline system for the \nWASSA-2017\n Shared Task on Emotion Intensity \n(EmoInt)\n. (Instructions for using the system with the task data are available \nhere\n.) Five participating teams used AffectiveTweets to generate feature vectors, including the teams that eventually ranked first, second, and third. \n\n\nDescription about the filters, installation instructions, and examples are given below.\n\n\nRelevant Papers\n\n\nThe most relevant papers on which this package is based are:\n\n\n\n\nSentiment Analysis of Short Informal Texts\n. Svetlana Kiritchenko, Xiaodan Zhu and Saif Mohammad. Journal of Artificial Intelligence Research, volume 50, pages 723-762, August 2014. \nBibTeX\n\n\nMeta-Level Sentiment Models for Big Social Data Analysis\n. F. Bravo-Marquez, M. Mendoza and B. Poblete. Knowledge-Based Systems Volume 69, October 2014, Pages 86\u201399. \nBibTex\n\n\nStance and sentiment in tweets\n. Saif M. Mohammad, Parinaz Sobhani, and Svetlana Kiritchenko. 2017. Special Section of the ACM Transactions on Internet Technology on Argumentation in Social Media 17(3). \nBibTeX\n\n\nSentiment strength detection for the social Web\n. Thelwall, M., Buckley, K., \n Paltoglou, G. (2012). Journal of the American Society for Information Science and Technology, 63(1), 163-173. \nBibTex\n\n\n\n\nCitation\n\n\nPlease cite the following paper if using this package in an academic publication:\n\n\n\n\nEmotion Intensities in Tweets\n. Saif M. Mohammad and Felipe Bravo-Marquez. In Proceedings of the Joint Conference on Lexical and Computational Semantics (*Sem), August 2017, Vancouver, Canada. \n\n\n\n\nYou should also cite the papers describing any of the lexicons or resources you are using with this package. \n\n\n\n\n\n\nHere is the \nBibTex\n entry for the package along with the entries for the resources listed below. \n\n\n\n\n\n\nHere is the \nBibTex\n entry just for the package.\n\n\n\n\n\n\nThe individual references for each resource can be found through the links provided below.\n\n\nFilters\n\n\nTweet-level Filters\n\n\n\n\n\n\nTweetToSparseFeatureVector\n: calculates sparse features, such as word and character n-grams from tweets. There are parameters for filtering out infrequent features e.g., (n-grams occurring in less than \nm\n tweets) and for setting the weighting approach  (boolean or frequency based).\n\n\n\n\nWord n-grams\n: extracts word n-grams from \nn\n=1 to a maximum value. \n\n\nNegations\n: add a prefix to words occurring in negated contexts, e.g., I don't like you =\n I don't NEG-like NEG-you. The prefixes only affect word n-gram features. The scope of negation finishes with the next punctuation expression \n([\\.|,|:|;|!|\\?]+)\n .\n\n\nCharacter n-grams\n: calculates character n-grams.\n\n\nPOS tags\n: tags tweets using the \nCMU Tweet NLP tool\n, and creates a vector space model based on the sequence of POS tags. \nBibTex\n\n\nBrown clusters\n: maps the words in a tweet to Brown word clusters and creates a low-dimensional vector space model. It can be used with n-grams of word clusters. The word clusters are also taken from the \nCMU Tweet NLP tool\n.\n\n\n\n\n\n\n\n\nTweetToLexiconFeatureVector\n: calculates features from a tweet using several lexicons.\n\n\n\n\nMPQA\n: counts the number of positive and negative words from the MPQA subjectivity lexicon. \nBibTex\n\n\nBing Liu\n: counts the number of positive and negative words from the Bing Liu lexicon. \nBibTex\n\n\nAFINN\n: calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon. \nBibTex\n\n\nSentiment140\n: calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon created with tweets annotated by emoticons. \nBibTex\n\n\nNRC Hashtag Sentiment lexicon\n: calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon created with tweets annotated with emotional hashtags. \nBibTex\n \n\n\nNRC Word-Emotion Association Lexicon\n: counts the number of words matching each emotion from this lexicon. \nBibTex\n\n\nNRC-10 Expanded\n: adds the emotion associations of the words matching the Twitter Specific expansion of the NRC Word-Emotion Association Lexicon. \nBibTex\n\n\nNRC Hashtag Emotion Association Lexicon\n: adds the emotion associations of the words matching this lexicon. \nBibTex\n  \n\n\nSentiWordNet\n: calculates positive and negative scores using SentiWordnet. We calculate a weighted average of the sentiment distributions of the synsets for word occurring in multiple synsets. The weights correspond to the reciprocal ranks of the senses in order to give higher weights to most popular senses. \nBibTex\n \n\n\nEmoticons\n: calculates a positive and a negative score by aggregating the word associations provided by a list of emoticons. The list is taken from the \nAFINN\n project.\n\n\nNegations: counts the number of negating words in the tweet.\n\n\n\n\n\n\n\n\nTweetToInputLexiconFeatureVector\n: calculates features from a tweet using a given list of affective lexicons, where each lexicon is represented as an \nARFF\n file.  The features are calculated by adding or counting the affective associations of the words matching the given lexicons. All numeric and nominal attributes from each lexicon are considered. Numeric scores are added and nominal are counted. The \nNRC-Affect-Intensity\n lexicon is used by deault.   \nBibTex\n\n\n\n\n\n\nTweetToSentiStrengthFeatureVector\n: calculates positive and negative sentiment strengths for a tweet using \nSentiStrength\n. Disclaimer: \nSentiStrength\n can only be used for academic purposes from within this package. \nBibTex\n\n\n\n\n\n\nTweetToEmbeddingsFeatureVector\n: calculates a tweet-level feature representation using pre-trained word embeddings. A dummy word-embedding formed by zeroes is used for word with no corresponding embedding. The tweet vectors can be calculated using the following schemes: \n\n\n\n\nAverage word embeddings.\n\n\nAdd word embeddings. \n\n\nConcatenation of first \nk\n embeddings. Dummy values are added if the tweet has less than \nk\n words. \n\n\n\n\n\n\n\n\nWord-level Filters\n\n\n\n\n\n\nPMILexiconExpander\n: calculates the Pointwise Mutual Information (PMI) semantic orientation for each word in a corpus of tweets annotated by sentiment. The score is calculated by substracting the PMI of the  target  word  with  a negative  sentiment from the PMI of the target word with a positive sentiment. This is a supervised filter.  \nBibTex\n \n\n\n\n\n\n\nTweetCentroid\n:  calculates word distributional vectors from a corpus of unlabelled tweets by treating them as the centroid of the tweet vectors in which they appear. The vectors can be labelled using an affective lexicon to train a word-level affective classifier. This classifier can be used to expand the original lexicon.  \nBibTex\n, \noriginal paper\n\n\n\n\n\n\nLabelWordVectors\n: labels word vectors with an input lexicon in arff format. This filter is useful for training word-level affective classifiers.\n\n\n\n\n\n\nDistant Supervision Filters\n\n\n\n\n\n\nASA\n:  Annotate-Sample-Average (ASA) is a lexicon-based distant supervision method for training polarity classifiers in Twitter in the absence of labelled data. It takes a collection of unlabelled tweets and a polarity lexicon in arff format and creates synthetic labelled instances. Each labelled instance is created by sampling with replacement a number of tweets containing at least one word from the lexicon with the desired polarity, and averaging the feature vectors of the sampled tweets.  \nBibTex\n, \noriginal paper\n\n\n\n\n\n\nPTCM\n:  The Partitioned Tweet Centroid Model (PTCM) is an adaption of the TweetCentroidModel for distant supervison.  As tweets and words are represented by the same feature vectors, a word-level classifier trained from a polarity lexicon and a corpus of unlabelled tweets can be used for classifying the sentiment of tweets represented by sparse feature vectors.  In other words, the labelled word vectors correspond to lexicon-annotated training data for message-level polarity classification.\nThe model includes a simple modification to the tweet centroid model for increasing the number of labelled instances, yielding \npartitioned tweet centroids\n.  This modification is based on partitioning the tweets associated with each word into smaller disjoint subsets of a fixed size. The method calculates one centroid per partition, which is labelled according to the lexicon.\n\nBibTex\n, \noriginal paper\n\n\n\n\n\n\nLexiconDistantSupervision\n: This is the most popolar distant supervision approach for Twitter sentiment analysis. It takes a collection of unlabelled tweets and a polarity lexicon in arff format of positive and negative tokens. If a word from the lexicon is found, the tweet is labelled with the word's polarity. Tweets with both positive and negative words are dicarded. The word used for labelling the tweet can be removed from the content. Emoticons are used as the default lexicon. \noriginal paper\n\n\n\n\n\n\nTokenizers\n\n\n\n\nTweetNLPTokenizer\n: a Twitter-specific String tokenizer based on the \nCMU Tweet NLP tool\n that can be used with the existing \nStringWordToVector\n Weka filter. \n\n\n\n\nOther Resources\n\n\n\n\nDatasets\n: The package provides some tweets annotated by affective values in gzipped \nARFF\n format in $WEKA_HOME/packages/AffectiveTweets/data/. The default location for $WEKA_HOME is $HOME/wekafiles. \n\n\n\n\nAffective Lexicons\n: The package provides affective lexicons in \nARFF\n format. These lexicons are located in $WEKA_HOME/packages/AffectiveTweets/lexicons/arff_lexicons/ and can be used with the  \nTweetToInputLexiconFeatureVector\n filter.\n\n\n\n\n\n\nPre-trained Word-Embeddings\n: The package provides a file with pre-trained word vectors trained with the \nWord2Vec\n tool in gzip compressed format. It is a tab separated file with the word in last column located in $WEKA_HOME/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz. However, this is a toy example trained from a small collection of tweets. We recommend downloading \nw2v.twitter.edinburgh10M.400d.csv.gz\n, which provides  embeddings trained from 10 million tweets taken from the \nEdinburgh corpus\n. The parameters were calibrated for classifying words into emotions. More info in this \npaper\n.\n\n\n\n\n\n\nDocumentation\n\n\nThe Java documentation is available \nhere\n.\n\n\nTeam\n\n\nMain Developer\n\n\n\n\nFelipe Bravo-Marquez\n\n\n\n\nContributors\n\n\n\n\nSaif Mohammad\n\n\nEibe Frank\n\n\nBernhard Pfahringer\n\n\n\n\nHow to Contribute\n\n\nNew contributors are more than welcome. If you want to contribute just fork the project and send a \npull request\n with your changes. \n\n\nContact\n\n\n\n\nEmail: fbravoma at waikato.ac.nz\n\n\nIf you have questions about Weka please refer to the Weka \nmailing list\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#about", 
            "text": "AffectiveTweets  is a  WEKA  package for analyzing emotion and sentiment of English written tweets.  The source code is hosted on  Github .  The package implements WEKA filters for calculating state-of-the-art affective analysis features from tweets that can be fed into machine learning algorithms. Many of these features were drawn from the  NRC-Canada System . It also implements methods for building affective lexicons and distant supervision methods for training affective models from unlabelled tweets.  The package was also made available as the official baseline system for the  WASSA-2017  Shared Task on Emotion Intensity  (EmoInt) . (Instructions for using the system with the task data are available  here .) Five participating teams used AffectiveTweets to generate feature vectors, including the teams that eventually ranked first, second, and third.   Description about the filters, installation instructions, and examples are given below.", 
            "title": "About"
        }, 
        {
            "location": "/#relevant-papers", 
            "text": "The most relevant papers on which this package is based are:   Sentiment Analysis of Short Informal Texts . Svetlana Kiritchenko, Xiaodan Zhu and Saif Mohammad. Journal of Artificial Intelligence Research, volume 50, pages 723-762, August 2014.  BibTeX  Meta-Level Sentiment Models for Big Social Data Analysis . F. Bravo-Marquez, M. Mendoza and B. Poblete. Knowledge-Based Systems Volume 69, October 2014, Pages 86\u201399.  BibTex  Stance and sentiment in tweets . Saif M. Mohammad, Parinaz Sobhani, and Svetlana Kiritchenko. 2017. Special Section of the ACM Transactions on Internet Technology on Argumentation in Social Media 17(3).  BibTeX  Sentiment strength detection for the social Web . Thelwall, M., Buckley, K.,   Paltoglou, G. (2012). Journal of the American Society for Information Science and Technology, 63(1), 163-173.  BibTex", 
            "title": "Relevant Papers"
        }, 
        {
            "location": "/#citation", 
            "text": "Please cite the following paper if using this package in an academic publication:   Emotion Intensities in Tweets . Saif M. Mohammad and Felipe Bravo-Marquez. In Proceedings of the Joint Conference on Lexical and Computational Semantics (*Sem), August 2017, Vancouver, Canada.    You should also cite the papers describing any of the lexicons or resources you are using with this package.     Here is the  BibTex  entry for the package along with the entries for the resources listed below.     Here is the  BibTex  entry just for the package.    The individual references for each resource can be found through the links provided below.", 
            "title": "Citation"
        }, 
        {
            "location": "/#filters", 
            "text": "", 
            "title": "Filters"
        }, 
        {
            "location": "/#tweet-level-filters", 
            "text": "TweetToSparseFeatureVector : calculates sparse features, such as word and character n-grams from tweets. There are parameters for filtering out infrequent features e.g., (n-grams occurring in less than  m  tweets) and for setting the weighting approach  (boolean or frequency based).   Word n-grams : extracts word n-grams from  n =1 to a maximum value.   Negations : add a prefix to words occurring in negated contexts, e.g., I don't like you =  I don't NEG-like NEG-you. The prefixes only affect word n-gram features. The scope of negation finishes with the next punctuation expression  ([\\.|,|:|;|!|\\?]+)  .  Character n-grams : calculates character n-grams.  POS tags : tags tweets using the  CMU Tweet NLP tool , and creates a vector space model based on the sequence of POS tags.  BibTex  Brown clusters : maps the words in a tweet to Brown word clusters and creates a low-dimensional vector space model. It can be used with n-grams of word clusters. The word clusters are also taken from the  CMU Tweet NLP tool .     TweetToLexiconFeatureVector : calculates features from a tweet using several lexicons.   MPQA : counts the number of positive and negative words from the MPQA subjectivity lexicon.  BibTex  Bing Liu : counts the number of positive and negative words from the Bing Liu lexicon.  BibTex  AFINN : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon.  BibTex  Sentiment140 : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon created with tweets annotated by emoticons.  BibTex  NRC Hashtag Sentiment lexicon : calculates positive and negative variables by aggregating the positive and negative word scores provided by this lexicon created with tweets annotated with emotional hashtags.  BibTex    NRC Word-Emotion Association Lexicon : counts the number of words matching each emotion from this lexicon.  BibTex  NRC-10 Expanded : adds the emotion associations of the words matching the Twitter Specific expansion of the NRC Word-Emotion Association Lexicon.  BibTex  NRC Hashtag Emotion Association Lexicon : adds the emotion associations of the words matching this lexicon.  BibTex     SentiWordNet : calculates positive and negative scores using SentiWordnet. We calculate a weighted average of the sentiment distributions of the synsets for word occurring in multiple synsets. The weights correspond to the reciprocal ranks of the senses in order to give higher weights to most popular senses.  BibTex    Emoticons : calculates a positive and a negative score by aggregating the word associations provided by a list of emoticons. The list is taken from the  AFINN  project.  Negations: counts the number of negating words in the tweet.     TweetToInputLexiconFeatureVector : calculates features from a tweet using a given list of affective lexicons, where each lexicon is represented as an  ARFF  file.  The features are calculated by adding or counting the affective associations of the words matching the given lexicons. All numeric and nominal attributes from each lexicon are considered. Numeric scores are added and nominal are counted. The  NRC-Affect-Intensity  lexicon is used by deault.    BibTex    TweetToSentiStrengthFeatureVector : calculates positive and negative sentiment strengths for a tweet using  SentiStrength . Disclaimer:  SentiStrength  can only be used for academic purposes from within this package.  BibTex    TweetToEmbeddingsFeatureVector : calculates a tweet-level feature representation using pre-trained word embeddings. A dummy word-embedding formed by zeroes is used for word with no corresponding embedding. The tweet vectors can be calculated using the following schemes:    Average word embeddings.  Add word embeddings.   Concatenation of first  k  embeddings. Dummy values are added if the tweet has less than  k  words.", 
            "title": "Tweet-level Filters"
        }, 
        {
            "location": "/#word-level-filters", 
            "text": "PMILexiconExpander : calculates the Pointwise Mutual Information (PMI) semantic orientation for each word in a corpus of tweets annotated by sentiment. The score is calculated by substracting the PMI of the  target  word  with  a negative  sentiment from the PMI of the target word with a positive sentiment. This is a supervised filter.   BibTex      TweetCentroid :  calculates word distributional vectors from a corpus of unlabelled tweets by treating them as the centroid of the tweet vectors in which they appear. The vectors can be labelled using an affective lexicon to train a word-level affective classifier. This classifier can be used to expand the original lexicon.   BibTex ,  original paper    LabelWordVectors : labels word vectors with an input lexicon in arff format. This filter is useful for training word-level affective classifiers.", 
            "title": "Word-level Filters"
        }, 
        {
            "location": "/#distant-supervision-filters", 
            "text": "ASA :  Annotate-Sample-Average (ASA) is a lexicon-based distant supervision method for training polarity classifiers in Twitter in the absence of labelled data. It takes a collection of unlabelled tweets and a polarity lexicon in arff format and creates synthetic labelled instances. Each labelled instance is created by sampling with replacement a number of tweets containing at least one word from the lexicon with the desired polarity, and averaging the feature vectors of the sampled tweets.   BibTex ,  original paper    PTCM :  The Partitioned Tweet Centroid Model (PTCM) is an adaption of the TweetCentroidModel for distant supervison.  As tweets and words are represented by the same feature vectors, a word-level classifier trained from a polarity lexicon and a corpus of unlabelled tweets can be used for classifying the sentiment of tweets represented by sparse feature vectors.  In other words, the labelled word vectors correspond to lexicon-annotated training data for message-level polarity classification.\nThe model includes a simple modification to the tweet centroid model for increasing the number of labelled instances, yielding  partitioned tweet centroids .  This modification is based on partitioning the tweets associated with each word into smaller disjoint subsets of a fixed size. The method calculates one centroid per partition, which is labelled according to the lexicon. BibTex ,  original paper    LexiconDistantSupervision : This is the most popolar distant supervision approach for Twitter sentiment analysis. It takes a collection of unlabelled tweets and a polarity lexicon in arff format of positive and negative tokens. If a word from the lexicon is found, the tweet is labelled with the word's polarity. Tweets with both positive and negative words are dicarded. The word used for labelling the tweet can be removed from the content. Emoticons are used as the default lexicon.  original paper", 
            "title": "Distant Supervision Filters"
        }, 
        {
            "location": "/#tokenizers", 
            "text": "TweetNLPTokenizer : a Twitter-specific String tokenizer based on the  CMU Tweet NLP tool  that can be used with the existing  StringWordToVector  Weka filter.", 
            "title": "Tokenizers"
        }, 
        {
            "location": "/#other-resources", 
            "text": "Datasets : The package provides some tweets annotated by affective values in gzipped  ARFF  format in $WEKA_HOME/packages/AffectiveTweets/data/. The default location for $WEKA_HOME is $HOME/wekafiles.    Affective Lexicons : The package provides affective lexicons in  ARFF  format. These lexicons are located in $WEKA_HOME/packages/AffectiveTweets/lexicons/arff_lexicons/ and can be used with the   TweetToInputLexiconFeatureVector  filter.    Pre-trained Word-Embeddings : The package provides a file with pre-trained word vectors trained with the  Word2Vec  tool in gzip compressed format. It is a tab separated file with the word in last column located in $WEKA_HOME/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz. However, this is a toy example trained from a small collection of tweets. We recommend downloading  w2v.twitter.edinburgh10M.400d.csv.gz , which provides  embeddings trained from 10 million tweets taken from the  Edinburgh corpus . The parameters were calibrated for classifying words into emotions. More info in this  paper .", 
            "title": "Other Resources"
        }, 
        {
            "location": "/#documentation", 
            "text": "The Java documentation is available  here .", 
            "title": "Documentation"
        }, 
        {
            "location": "/#team", 
            "text": "", 
            "title": "Team"
        }, 
        {
            "location": "/#main-developer", 
            "text": "Felipe Bravo-Marquez", 
            "title": "Main Developer"
        }, 
        {
            "location": "/#contributors", 
            "text": "Saif Mohammad  Eibe Frank  Bernhard Pfahringer", 
            "title": "Contributors"
        }, 
        {
            "location": "/#how-to-contribute", 
            "text": "New contributors are more than welcome. If you want to contribute just fork the project and send a  pull request  with your changes.", 
            "title": "How to Contribute"
        }, 
        {
            "location": "/#contact", 
            "text": "Email: fbravoma at waikato.ac.nz  If you have questions about Weka please refer to the Weka  mailing list .", 
            "title": "Contact"
        }, 
        {
            "location": "/install/", 
            "text": "Installing  Weka\n\n\nDownload the latest stable \nversion\n or the  developer \nbranch\n of Weka.\nYou can also build the developer branch from the SVN repository: \n\n\n# checkout weka \nsvn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka/\n# build weka using apache ant\nant -f weka/build.xml exejar\n\n\n\n\nInstalling AffectiveTweets\n\n\nInstall AffectiveTweets1.0.0 using the \nWekaPackageManager\n: \n\n\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package AffectiveTweets\n\n\n\n\nIn order to properly run our \nexamples\n, we recommend installing the newest version of the package v.1.0.1 (not officially released yet) as follows: \n\n\n# Uninstall the previous version of AffectiveTweets\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -uninstall-package AffectiveTweets\n# Install the newest development version:\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package https://github.com/felipebravom/AffectiveTweets/releases/download/1.0.1/AffectiveTweets1.0.1.zip\n\n\n\n\nBuilding AffectiveTweets\n\n\nYou can also build the package from the repository's version to try the most recent features. This is very useful if you want to contribute.\n\n\n# clone the repository\ngit clone https://github.com/felipebravom/AffectiveTweets.git\ncd AffectiveTweets\n\n# Download additional files\nwget https://github.com/felipebravom/AffectiveTweets/releases/download/1.0.1/extra.zip\nunzip extra.zip\n\n# Build the package using apache ant\nant -f build_package.xml make_package\n\n# Install the built package \njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package dist/AffectiveTweets.zip\n\n\n\n\n\n\nOther Useful Packages\n\n\nWe recommend installing other useful packages for classification, regression and evaluation:\n\n\n\n\nLibLinear\n: This package is required for running the \nexamples\n.\n\n\n\n\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package LibLINEAR\n\n\n\n\n\n\nLibSVM\n\n\n\n\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package LibSVM\n\n\n\n\n\n\nRankCorrelation\n\n\n\n\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package RankCorrelation\n\n\n\n\n\n\nSnowball-stemmers\n: This package allows using the Porter stemmer as well as other Snowball stemmers.\n\n\n\n\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package https://github.com/fracpete/snowball-stemmers-weka-package/releases/download/v1.0.1/snowball-stemmers-1.0.1.zip\n\n\n\n\n\n\nThe \nWekaDeepLearning4j\n package can be installed for training deep neural networks and word embeddings.", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installing-weka", 
            "text": "Download the latest stable  version  or the  developer  branch  of Weka.\nYou can also build the developer branch from the SVN repository:   # checkout weka \nsvn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka/\n# build weka using apache ant\nant -f weka/build.xml exejar", 
            "title": "Installing  Weka"
        }, 
        {
            "location": "/install/#installing-affectivetweets", 
            "text": "Install AffectiveTweets1.0.0 using the  WekaPackageManager :   java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package AffectiveTweets  In order to properly run our  examples , we recommend installing the newest version of the package v.1.0.1 (not officially released yet) as follows:   # Uninstall the previous version of AffectiveTweets\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -uninstall-package AffectiveTweets\n# Install the newest development version:\njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package https://github.com/felipebravom/AffectiveTweets/releases/download/1.0.1/AffectiveTweets1.0.1.zip", 
            "title": "Installing AffectiveTweets"
        }, 
        {
            "location": "/install/#building-affectivetweets", 
            "text": "You can also build the package from the repository's version to try the most recent features. This is very useful if you want to contribute.  # clone the repository\ngit clone https://github.com/felipebravom/AffectiveTweets.git\ncd AffectiveTweets\n\n# Download additional files\nwget https://github.com/felipebravom/AffectiveTweets/releases/download/1.0.1/extra.zip\nunzip extra.zip\n\n# Build the package using apache ant\nant -f build_package.xml make_package\n\n# Install the built package \njava -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package dist/AffectiveTweets.zip", 
            "title": "Building AffectiveTweets"
        }, 
        {
            "location": "/install/#other-useful-packages", 
            "text": "We recommend installing other useful packages for classification, regression and evaluation:   LibLinear : This package is required for running the  examples .   java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package LibLINEAR   LibSVM   java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package LibSVM   RankCorrelation   java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package RankCorrelation   Snowball-stemmers : This package allows using the Porter stemmer as well as other Snowball stemmers.   java -cp $WEKA_PATH/weka.jar weka.core.WekaPackageManager -install-package https://github.com/fracpete/snowball-stemmers-weka-package/releases/download/v1.0.1/snowball-stemmers-1.0.1.zip   The  WekaDeepLearning4j  package can be installed for training deep neural networks and word embeddings.", 
            "title": "Other Useful Packages"
        }, 
        {
            "location": "/examples/", 
            "text": "The package can be used from the Weka GUI or the command line.\n\n\nNote: The following examples work with the newest version of the package. \n\n\nGUI\n\n\nRun WEKA and open the Explorer:  \n\n\n java -Xmx4G -jar weka.jar \n\n\n\n\nNote: The -Xmx parameter allows incrementing the memory available for the Java virtual machine. It is strongly recommend to allocate as much memory as possible for large datasets or when calculating large dimensional features, such as word n-grams. More info \nhere\n.\n\n\nTrain an SVM using sparse features\n\n\n\n\n\n\nOpen in the preprocess panel the \nsent140test.arff.gz\n dataset located in HOME/wekafiles/packages/AffectiveTweets/data/. Note: Select arff.gz files in the \nFiles of Type\n option. \n\n\n\n\n\n\nChoose the \nTweetToSparseFeatureVector\n filter and configure it for calculating word n-grams, character n-grams, Brown word clusters, and POS tags:\n\n\n\n\n\n\n\n\n\n\n\n\nTrain an SVM using LibLinear. Go to the \nclassify\n panel and select the target class as the variable (Nom) class. \n\n\n\n\n\n\nRight click on the panel right to the \nChoose\n button and click on the \nEdit Configuration option\n. Paste the following snippet:\n\n\n\n\n\n\n weka.classifiers.meta.FilteredClassifier -F \nweka.filters.unsupervised.attribute.RemoveType -T string\n -W    weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000\n\n\n\n\nNote: Weka allows copying and pasting the configuration of its objets. This is very convenient when training complex schemes with various parameters.  The FilteredClassfier allows directly  passing a filter to the classifier. In this example, we are removing the attributes of type string.\n\n\n\n\nSelect the Percentage split option and start training the classifier \n\n\n\n\nTrain an SVM using multiple opinion lexicons, SentiStrength, and the average word-embedding vector\n\n\n\n\nGo back to the preprocess panel and press the \nUndo\n button to go back to the original dataset (or load the \nsent140test.arff.gz\n dataset in case you skipped the first example).\n\n\nGo to the \nClassify\n panel and paste the following snippet in the classifier's configuration:\n\n\n\n\nweka.classifiers.meta.FilteredClassifier -F  \nweka.filters.MultiFilter -F \\\nweka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $HOME/wekafiles/packages/AffectiveTweets/lexicons/SentiStrength/english -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\nweka.core.stopwords.Null \\\\\\\n -I 1 -U -tokenizer \\\\\\\nweka.core.tokenizers.TweetNLPTokenizer \\\\\\\n\\\n -F \\\nweka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -T -L -N -P -J -H -Q -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\nweka.core.stopwords.Null \\\\\\\n -I 1 -U -tokenizer \\\\\\\nweka.core.tokenizers.TweetNLPTokenizer \\\\\\\n\\\n -F \\\nweka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S AVERAGE_ACTION -embeddingHandler \\\\\\\naffective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep \\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\n -I last\\\\\\\n -K 15 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\nweka.core.stopwords.Null \\\\\\\n -I 1 -U -tokenizer \\\\\\\nweka.core.tokenizers.TweetNLPTokenizer \\\\\\\n\\\n -F \\\nweka.filters.unsupervised.attribute.Reorder -R 4-last,3\\\n -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000\n\n\n\n\nNote: replace $HOME by your home directory (e.g., /home/felipe). \n\n\n\n\n\n\nWe are using the MultiFilter filter to nest multiple filters.  The Reorder filter is used to discard the first two String attributes and moving the class label to the last position.\n\n\n\n\n\n\nNow you can train the classifier by pressing the \nStart\n button. \n\n\n\n\n\n\nTrain a Convolution Neural Network on the concatenation of word embeddings\n\n\nIn this example we will show how to train a convolution neural network with a similar arquitecture to the one used in this \npaper\n using the \nWekaDeepLearning4j\n package, which is a wrapper of the \nDeepLearning4j\n library. \n\n\n\n\n\n\nFirst, install the package following the instructions from \nhere\n:\n\n\n\n\n\n\nRepresent each tweet from the \nsent140test.arff.gz\n  dataset as a sequence of its first 15 word embeddings by pasting the following filter configuration:\n\n\n\n\n\n\nweka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S CONCATENATE_ACTION -embeddingHandler \naffective.core.CSVEmbeddingHandler -K $HOME/wekafiles/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep \\\n\\\\t\\\n -I last\n -K 15 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \nweka.core.stopwords.Null \n -I 1 -U -tokenizer \nweka.core.tokenizers.TweetNLPTokenizer \n\n\n\n\n\n\n\n\nDiscard the string content and move the class label to the last position:\n\n\n\n\nweka.filters.unsupervised.attribute.Reorder -R 4-last,3\n\n\n\n\n\n\nTrain a convolutional neural network using a \nDl4jMlpClassifier\n. Paste the following snippet in the Classification panel: \n\n\n\n\n weka.classifiers.functions.Dl4jMlpClassifier -S 1 -iterator \nweka.dl4j.iterators.ConvolutionalInstancesIterator -height 1 -numChannels 1 -bs 256 -width 1500\n -layers \nweka.dl4j.layers.ConvolutionLayer -nFilters 100 -activation identity -adamMeanDecay 0.9 -adamVarDecay 0.999 -biasInit 1.0 -biasL1 0.0 -biasL2 0.0 -blr 0.01 -mode Truncate -cudnnAlgoMode PREFER_FASTEST -dist \\\nweka.dl4j.distribution.NormalDistribution -mean 0.001 -std 1.0\\\n -dropout 0.0 -epsilon 1.0E-6 -gradientNormalization None -gradNormThreshold 1.0 -kernelSizeX 300 -kernelSizeY 1 -L1 0.0 -L2 0.0 -name \\\nConvolution layer\\\n -lr 0.01 -momentum 0.9 -paddingX 0 -paddingY 0 -rho 0.0 -rmsDecay 0.95 -strideX 100 -strideY 1 -updater NESTEROVS -weightInit XAVIER\n -layers \nweka.dl4j.layers.OutputLayer -activation softmax -adamMeanDecay 0.9 -adamVarDecay 0.999 -biasInit 1.0 -biasL1 0.0 -biasL2 0.0 -blr 0.01 -dist \\\nweka.dl4j.distribution.NormalDistribution -mean 0.001 -std 1.0\\\n -dropout 0.0 -epsilon 1.0E-6 -gradientNormalization None -gradNormThreshold 1.0 -L1 0.0 -L2 0.0 -name \\\nOutput layer\\\n -lr 0.01 -lossFn LossMCXENT() -momentum 0.9 -rho 0.0 -rmsDecay 0.95 -updater NESTEROVS -weightInit XAVIER\n -logFile weka.log -numEpochs 200 -algorithm STOCHASTIC_GRADIENT_DESCENT\n\n\n\n\nThis network has 100 filters in a convolutional layer, followed by the output layer. The filter size is 300x1 (i.e, each filter maps a word trigram, since each word has 100 dimensions). The stride is 100x1 (the number of dimensions for a word). The number of epochs is 200. The input width is 1500 and the input height is 1. The number of input channels is 1 and the batch size is 256.\n\n\nCreate a Lexicon of sentiment words using the TweetCentroid method\n\n\n\n\n\n\nOpen in the preprocess panel the \nunlabelled.arff.gz\n dataset of unlabelled tweets. \n\n\n\n\n\n\nTrain word vectors using the tweet centroid model using the TweetCentroid filter. Paste the following snippet:\n\n\n\n\n\n\nweka.filters.unsupervised.attribute.TweetCentroid -C -W -F -natt -M 10 -N 10 -H $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \nweka.core.stopwords.Null \n -I 1 -U -tokenizer \nweka.core.tokenizers.TweetNLPTokenizer \n\n\n\n\n\n\n\n\n\nLabel the resulting word vectors with a seed lexicon in arff format using the LabelWordVector Filter:\n\n\n\n\nweka.filters.unsupervised.attribute.LabelWordVectors -lexicon_evaluator \naffective.core.ArffLexiconWordLabeller -lexiconFile $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/metaLexEmo.arff -B MetaLexEmo -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\n -U -I last\n\n\n\n\n\n\nTrain a classifier a logistic regression on labelled words and add predictions as new attributes using the AddClassification filter:\n\n\n\n\nweka.filters.supervised.attribute.AddClassification -remove-old-class -distribution -W \nweka.classifiers.meta.FilteredClassifier -F \\\nweka.filters.unsupervised.attribute.RemoveType -T string\\\n -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000\n\n\n\n\n\n\n\nRemove all the word attributes to create a lexicon:\n\n\n\n\nweka.filters.unsupervised.attribute.Remove -R first-4121\n\n\n\n\n\n\n\n\nSave the resulting lexicon as an arff file by clicking on the save button.\n\n\n\n\n\n\nUse your new lexicon on a different tweet dataset using the \nTweetToInputLexiconFeatureVector\n filter.\n\n\n\n\n\n\nCreate a Lexicon of sentiment words using PMI Semantic Orientation\n\n\n\n\n\n\nOpen in the preprocess panel the \nsent140train.arff.gz\n dataset. This is a large corpus, so make sure to increase the heap size when running Weka.\n\n\n\n\n\n\nCreate a PMI lexicon using the PMILexiconExpander filter with default parameters. This is a supervised filter.\n\n\n\n\n\n\nSave the lexicon as an arff file and use it with the \nTweetToInputLexiconFeatureVector\n filter.\n\n\n\n\n\n\nTrain a Tweet-level polarity classifier from unlabelled tweets using emoticon labels\n\n\nDistant supervision is very useful when tweets annotated by sentiment are not available. In this example we will show how to train a classifier using emoticons as noisy labels.\n\n\n\n\nOpen in the preprocess panel the \nunlabelled.arff.gz\n dataset of unlabelled tweets. \n\n\nLabel tweets based on the polarity of emoticons (tweets without emoticons will be discarded):\n\n\n\n\nweka.filters.unsupervised.attribute.LexiconDistantSupervision -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/emoticons.arff -polatt polarity -negval negative -posval positive -removeMatchingWord -I 1 -tokenizer \nweka.core.tokenizers.TweetNLPTokenizer \n \n\n\n\n\n\n\nRename the polarity label to \nclass\n (this is needed to make the data compatible with the testing set):\n\n\n\n\nweka.filters.unsupervised.attribute.RenameAttribute -find polarity -replace class -R last\n\n\n\n\n\n\nTrain a classifier using unigram as features and deploy the classifier on target annotated tweets. Go to the classsify panel and set the file \n6HumanPosNeg.arff.gz\n as the supplied test set. Next, paste the following snippet in the classify panel:\n\n\n\n\nweka.classifiers.meta.FilteredClassifier -F \nweka.filters.MultiFilter -F \\\nweka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 0 -G 0 -taggerFile $HOME/wekafiles/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 1 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\nweka.core.stopwords.Null \\\\\\\n -I 1 -U -tokenizer \\\\\\\nweka.core.tokenizers.TweetNLPTokenizer \\\\\\\n\\\n -F \\\nweka.filters.unsupervised.attribute.Reorder -R 3-last,2\\\n -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000\n\n\n\n\nTrain a Tweet-level polarity classifier from unlabelled tweets using the ASA and PTCM distant supervision methods\n\n\nIn this example we will generate positive and negative instances from a corpus of unlabelled tweets using the ASA and the PTCM methods. The classifier wil be evaluated on positive and negative tweets.\n\n\n\n\nOpen in the preprocess panel the \nunlabelled.arff.gz\n dataset of unlabelled tweets. \n\n\nAdd a class label with negative and positive values using the Add filter in the preprocess panel:\n\n\n\n\nweka.filters.unsupervised.attribute.Add -T NOM -N class -L negative,positive -C last\n\n\n\n\nNote that the values for the class are empty for all instances. We are adding these labels to make the data compatible with the target tweets on which the classifier we will train will be deployed.\n\n\n\n\nGenerate positive and negative instances using ASA and the BingLiu lexicon, then train a logistic regression on those instances, and deploy this classifier on the tweets from \n6HumanPosNeg.arff.gz\n. Go to the classsify panel and set the file \n6HumanPosNeg.arff.gz\n as the supplied test set. Next, paste the following snippet in the classify panel:\n\n\n\n\nweka.classifiers.meta.FilteredClassifier -F \nweka.filters.unsupervised.attribute.ASA -C -W -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/BingLiu.arff -M 10 -nneg 1000 -npos 1000 -polatt polarity -negval negative -posval positive -R 1 -A 10 -H $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\nweka.core.stopwords.Null \\\n -I 1 -U -tokenizer \\\nweka.core.tokenizers.TweetNLPTokenizer \\\n -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000\n\n\n\n\n\n\nPaste the following snippet for using the PTCM. The partition size for the word vectors is set to 4: \n\n\n\n\nweka.classifiers.meta.FilteredClassifier -F \nweka.filters.unsupervised.attribute.PTCM -C -W -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/BingLiu.arff -M 4 -N 4 -A 10 -H /Users/admin/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\nweka.core.stopwords.Null \\\n -I 1 -U -tokenizer \\\nweka.core.tokenizers.TweetNLPTokenizer \\\n -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000\n\n\n\n\nCommand-line\n\n\nThe same classification schemes can be run from the command line. \n\n\nTweet Classification from the CL\n\n\nAn example using word embeddings is given below:\n\n\njava -Xmx4G -cp weka.jar weka.Run weka.classifiers.meta.FilteredClassifier  -t $HOME/wekafiles/packages/AffectiveTweets/data/sent140test.arff.gz -split-percentage 66 -F  \nweka.filters.MultiFilter -F \\\nweka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $HOME/wekafiles/packages/AffectiveTweets/lexicons/SentiStrength/english -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\nweka.core.stopwords.Null \\\\\\\n -I 1 -U -tokenizer \\\\\\\nweka.core.tokenizers.TweetNLPTokenizer \\\\\\\n\\\n -F \\\nweka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -T -L -N -P -J -H -Q -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\nweka.core.stopwords.Null \\\\\\\n -I 1 -U -tokenizer \\\\\\\nweka.core.tokenizers.TweetNLPTokenizer \\\\\\\n\\\n -F \\\nweka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S AVERAGE_ACTION -embeddingHandler \\\\\\\naffective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep \\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\n -I last\\\\\\\n -K 15 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\\nweka.core.stopwords.Null \\\\\\\n -I 1 -U -tokenizer \\\\\\\nweka.core.tokenizers.TweetNLPTokenizer \\\\\\\n\\\n -F \\\nweka.filters.unsupervised.attribute.Reorder -R 4-last,3\\\n -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000\n\n\n\n\nFeature Extraction from the CL\n\n\nThere is also possible to run filters in isolation and then convert the processed files into CSV files:\n\n\n\n\nFirst run a filter:\n\n\n\n\njava -Xmx4G -cp weka.jar weka.Run weka.filters.unsupervised.attribute.TweetToInputLexiconFeatureVector  -i  $HOME/wekafiles/packages/AffectiveTweets/data/sent140test.arff.gz -o proc_data.arff -lexicon_evaluator \naffective.core.ArffLexiconEvaluator -lexiconFile /Users/admin/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/NRC-AffectIntensity-Lexicon.arff -B NRC-Affect-Intensity -A 1 -lex-stemmer weka.core.stemmers.NullStemmer\n -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \nweka.core.stopwords.Null \n -I 1 -U -tokenizer \nweka.core.tokenizers.TweetNLPTokenizer \n\n\n\n\n\n\n\nThen, convert the new feature vector into a CSV file.\n\n\n\n\njava -Xmx4G -cp weka.jar weka.core.converters.CSVSaver -i proc_data.arff -o proc_data.csv \n\n\n\n\nMore information about how to run filters from the command line on the test data can be found \nhere\n.", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#gui", 
            "text": "Run WEKA and open the Explorer:     java -Xmx4G -jar weka.jar   Note: The -Xmx parameter allows incrementing the memory available for the Java virtual machine. It is strongly recommend to allocate as much memory as possible for large datasets or when calculating large dimensional features, such as word n-grams. More info  here .", 
            "title": "GUI"
        }, 
        {
            "location": "/examples/#train-an-svm-using-sparse-features", 
            "text": "Open in the preprocess panel the  sent140test.arff.gz  dataset located in HOME/wekafiles/packages/AffectiveTweets/data/. Note: Select arff.gz files in the  Files of Type  option.     Choose the  TweetToSparseFeatureVector  filter and configure it for calculating word n-grams, character n-grams, Brown word clusters, and POS tags:       Train an SVM using LibLinear. Go to the  classify  panel and select the target class as the variable (Nom) class.     Right click on the panel right to the  Choose  button and click on the  Edit Configuration option . Paste the following snippet:     weka.classifiers.meta.FilteredClassifier -F  weka.filters.unsupervised.attribute.RemoveType -T string  -W    weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000  Note: Weka allows copying and pasting the configuration of its objets. This is very convenient when training complex schemes with various parameters.  The FilteredClassfier allows directly  passing a filter to the classifier. In this example, we are removing the attributes of type string.   Select the Percentage split option and start training the classifier", 
            "title": "Train an SVM using sparse features"
        }, 
        {
            "location": "/examples/#train-an-svm-using-multiple-opinion-lexicons-sentistrength-and-the-average-word-embedding-vector", 
            "text": "Go back to the preprocess panel and press the  Undo  button to go back to the original dataset (or load the  sent140test.arff.gz  dataset in case you skipped the first example).  Go to the  Classify  panel and paste the following snippet in the classifier's configuration:   weka.classifiers.meta.FilteredClassifier -F   weka.filters.MultiFilter -F \\ weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $HOME/wekafiles/packages/AffectiveTweets/lexicons/SentiStrength/english -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\ weka.core.stopwords.Null \\\\\\  -I 1 -U -tokenizer \\\\\\ weka.core.tokenizers.TweetNLPTokenizer \\\\\\ \\  -F \\ weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -T -L -N -P -J -H -Q -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\ weka.core.stopwords.Null \\\\\\  -I 1 -U -tokenizer \\\\\\ weka.core.tokenizers.TweetNLPTokenizer \\\\\\ \\  -F \\ weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S AVERAGE_ACTION -embeddingHandler \\\\\\ affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep \\\\\\\\\\\\\\ \\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\  -I last\\\\\\  -K 15 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\ weka.core.stopwords.Null \\\\\\  -I 1 -U -tokenizer \\\\\\ weka.core.tokenizers.TweetNLPTokenizer \\\\\\ \\  -F \\ weka.filters.unsupervised.attribute.Reorder -R 4-last,3\\  -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000  Note: replace $HOME by your home directory (e.g., /home/felipe).     We are using the MultiFilter filter to nest multiple filters.  The Reorder filter is used to discard the first two String attributes and moving the class label to the last position.    Now you can train the classifier by pressing the  Start  button.", 
            "title": "Train an SVM using multiple opinion lexicons, SentiStrength, and the average word-embedding vector"
        }, 
        {
            "location": "/examples/#train-a-convolution-neural-network-on-the-concatenation-of-word-embeddings", 
            "text": "In this example we will show how to train a convolution neural network with a similar arquitecture to the one used in this  paper  using the  WekaDeepLearning4j  package, which is a wrapper of the  DeepLearning4j  library.     First, install the package following the instructions from  here :    Represent each tweet from the  sent140test.arff.gz   dataset as a sequence of its first 15 word embeddings by pasting the following filter configuration:    weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S CONCATENATE_ACTION -embeddingHandler  affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep \\ \\\\t\\  -I last  -K 15 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler  weka.core.stopwords.Null   -I 1 -U -tokenizer  weka.core.tokenizers.TweetNLPTokenizer     Discard the string content and move the class label to the last position:   weka.filters.unsupervised.attribute.Reorder -R 4-last,3   Train a convolutional neural network using a  Dl4jMlpClassifier . Paste the following snippet in the Classification panel:     weka.classifiers.functions.Dl4jMlpClassifier -S 1 -iterator  weka.dl4j.iterators.ConvolutionalInstancesIterator -height 1 -numChannels 1 -bs 256 -width 1500  -layers  weka.dl4j.layers.ConvolutionLayer -nFilters 100 -activation identity -adamMeanDecay 0.9 -adamVarDecay 0.999 -biasInit 1.0 -biasL1 0.0 -biasL2 0.0 -blr 0.01 -mode Truncate -cudnnAlgoMode PREFER_FASTEST -dist \\ weka.dl4j.distribution.NormalDistribution -mean 0.001 -std 1.0\\  -dropout 0.0 -epsilon 1.0E-6 -gradientNormalization None -gradNormThreshold 1.0 -kernelSizeX 300 -kernelSizeY 1 -L1 0.0 -L2 0.0 -name \\ Convolution layer\\  -lr 0.01 -momentum 0.9 -paddingX 0 -paddingY 0 -rho 0.0 -rmsDecay 0.95 -strideX 100 -strideY 1 -updater NESTEROVS -weightInit XAVIER  -layers  weka.dl4j.layers.OutputLayer -activation softmax -adamMeanDecay 0.9 -adamVarDecay 0.999 -biasInit 1.0 -biasL1 0.0 -biasL2 0.0 -blr 0.01 -dist \\ weka.dl4j.distribution.NormalDistribution -mean 0.001 -std 1.0\\  -dropout 0.0 -epsilon 1.0E-6 -gradientNormalization None -gradNormThreshold 1.0 -L1 0.0 -L2 0.0 -name \\ Output layer\\  -lr 0.01 -lossFn LossMCXENT() -momentum 0.9 -rho 0.0 -rmsDecay 0.95 -updater NESTEROVS -weightInit XAVIER  -logFile weka.log -numEpochs 200 -algorithm STOCHASTIC_GRADIENT_DESCENT  This network has 100 filters in a convolutional layer, followed by the output layer. The filter size is 300x1 (i.e, each filter maps a word trigram, since each word has 100 dimensions). The stride is 100x1 (the number of dimensions for a word). The number of epochs is 200. The input width is 1500 and the input height is 1. The number of input channels is 1 and the batch size is 256.", 
            "title": "Train a Convolution Neural Network on the concatenation of word embeddings"
        }, 
        {
            "location": "/examples/#create-a-lexicon-of-sentiment-words-using-the-tweetcentroid-method", 
            "text": "Open in the preprocess panel the  unlabelled.arff.gz  dataset of unlabelled tweets.     Train word vectors using the tweet centroid model using the TweetCentroid filter. Paste the following snippet:    weka.filters.unsupervised.attribute.TweetCentroid -C -W -F -natt -M 10 -N 10 -H $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler  weka.core.stopwords.Null   -I 1 -U -tokenizer  weka.core.tokenizers.TweetNLPTokenizer     Label the resulting word vectors with a seed lexicon in arff format using the LabelWordVector Filter:   weka.filters.unsupervised.attribute.LabelWordVectors -lexicon_evaluator  affective.core.ArffLexiconWordLabeller -lexiconFile $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/metaLexEmo.arff -B MetaLexEmo -A 1 -lex-stemmer weka.core.stemmers.NullStemmer  -U -I last   Train a classifier a logistic regression on labelled words and add predictions as new attributes using the AddClassification filter:   weka.filters.supervised.attribute.AddClassification -remove-old-class -distribution -W  weka.classifiers.meta.FilteredClassifier -F \\ weka.filters.unsupervised.attribute.RemoveType -T string\\  -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000    Remove all the word attributes to create a lexicon:   weka.filters.unsupervised.attribute.Remove -R first-4121    Save the resulting lexicon as an arff file by clicking on the save button.    Use your new lexicon on a different tweet dataset using the  TweetToInputLexiconFeatureVector  filter.", 
            "title": "Create a Lexicon of sentiment words using the TweetCentroid method"
        }, 
        {
            "location": "/examples/#create-a-lexicon-of-sentiment-words-using-pmi-semantic-orientation", 
            "text": "Open in the preprocess panel the  sent140train.arff.gz  dataset. This is a large corpus, so make sure to increase the heap size when running Weka.    Create a PMI lexicon using the PMILexiconExpander filter with default parameters. This is a supervised filter.    Save the lexicon as an arff file and use it with the  TweetToInputLexiconFeatureVector  filter.", 
            "title": "Create a Lexicon of sentiment words using PMI Semantic Orientation"
        }, 
        {
            "location": "/examples/#train-a-tweet-level-polarity-classifier-from-unlabelled-tweets-using-emoticon-labels", 
            "text": "Distant supervision is very useful when tweets annotated by sentiment are not available. In this example we will show how to train a classifier using emoticons as noisy labels.   Open in the preprocess panel the  unlabelled.arff.gz  dataset of unlabelled tweets.   Label tweets based on the polarity of emoticons (tweets without emoticons will be discarded):   weka.filters.unsupervised.attribute.LexiconDistantSupervision -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/emoticons.arff -polatt polarity -negval negative -posval positive -removeMatchingWord -I 1 -tokenizer  weka.core.tokenizers.TweetNLPTokenizer      Rename the polarity label to  class  (this is needed to make the data compatible with the testing set):   weka.filters.unsupervised.attribute.RenameAttribute -find polarity -replace class -R last   Train a classifier using unigram as features and deploy the classifier on target annotated tweets. Go to the classsify panel and set the file  6HumanPosNeg.arff.gz  as the supplied test set. Next, paste the following snippet in the classify panel:   weka.classifiers.meta.FilteredClassifier -F  weka.filters.MultiFilter -F \\ weka.filters.unsupervised.attribute.TweetToSparseFeatureVector -E 5 -D 3 -I 0 -F -M 0 -G 0 -taggerFile $HOME/wekafiles/packages/AffectiveTweets/resources/model.20120919 -wordClustFile $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -Q 1 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\ weka.core.stopwords.Null \\\\\\  -I 1 -U -tokenizer \\\\\\ weka.core.tokenizers.TweetNLPTokenizer \\\\\\ \\  -F \\ weka.filters.unsupervised.attribute.Reorder -R 3-last,2\\  -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000", 
            "title": "Train a Tweet-level polarity classifier from unlabelled tweets using emoticon labels"
        }, 
        {
            "location": "/examples/#train-a-tweet-level-polarity-classifier-from-unlabelled-tweets-using-the-asa-and-ptcm-distant-supervision-methods", 
            "text": "In this example we will generate positive and negative instances from a corpus of unlabelled tweets using the ASA and the PTCM methods. The classifier wil be evaluated on positive and negative tweets.   Open in the preprocess panel the  unlabelled.arff.gz  dataset of unlabelled tweets.   Add a class label with negative and positive values using the Add filter in the preprocess panel:   weka.filters.unsupervised.attribute.Add -T NOM -N class -L negative,positive -C last  Note that the values for the class are empty for all instances. We are adding these labels to make the data compatible with the target tweets on which the classifier we will train will be deployed.   Generate positive and negative instances using ASA and the BingLiu lexicon, then train a logistic regression on those instances, and deploy this classifier on the tweets from  6HumanPosNeg.arff.gz . Go to the classsify panel and set the file  6HumanPosNeg.arff.gz  as the supplied test set. Next, paste the following snippet in the classify panel:   weka.classifiers.meta.FilteredClassifier -F  weka.filters.unsupervised.attribute.ASA -C -W -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/BingLiu.arff -M 10 -nneg 1000 -npos 1000 -polatt polarity -negval negative -posval positive -R 1 -A 10 -H $HOME/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\ weka.core.stopwords.Null \\  -I 1 -U -tokenizer \\ weka.core.tokenizers.TweetNLPTokenizer \\  -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000   Paste the following snippet for using the PTCM. The partition size for the word vectors is set to 4:    weka.classifiers.meta.FilteredClassifier -F  weka.filters.unsupervised.attribute.PTCM -C -W -lex $HOME/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/BingLiu.arff -M 4 -N 4 -A 10 -H /Users/admin/wekafiles/packages/AffectiveTweets/resources/50mpaths2.txt.gz -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\ weka.core.stopwords.Null \\  -I 1 -U -tokenizer \\ weka.core.tokenizers.TweetNLPTokenizer \\  -W weka.classifiers.functions.LibLINEAR -- -S 7 -C 1.0 -E 0.001 -B 1.0 -P -L 0.1 -I 1000", 
            "title": "Train a Tweet-level polarity classifier from unlabelled tweets using the ASA and PTCM distant supervision methods"
        }, 
        {
            "location": "/examples/#command-line", 
            "text": "The same classification schemes can be run from the command line.", 
            "title": "Command-line"
        }, 
        {
            "location": "/examples/#tweet-classification-from-the-cl", 
            "text": "An example using word embeddings is given below:  java -Xmx4G -cp weka.jar weka.Run weka.classifiers.meta.FilteredClassifier  -t $HOME/wekafiles/packages/AffectiveTweets/data/sent140test.arff.gz -split-percentage 66 -F   weka.filters.MultiFilter -F \\ weka.filters.unsupervised.attribute.TweetToSentiStrengthFeatureVector -L $HOME/wekafiles/packages/AffectiveTweets/lexicons/SentiStrength/english -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\ weka.core.stopwords.Null \\\\\\  -I 1 -U -tokenizer \\\\\\ weka.core.tokenizers.TweetNLPTokenizer \\\\\\ \\  -F \\ weka.filters.unsupervised.attribute.TweetToLexiconFeatureVector -F -D -R -A -T -L -N -P -J -H -Q -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\ weka.core.stopwords.Null \\\\\\  -I 1 -U -tokenizer \\\\\\ weka.core.tokenizers.TweetNLPTokenizer \\\\\\ \\  -F \\ weka.filters.unsupervised.attribute.TweetToEmbeddingsFeatureVector -S AVERAGE_ACTION -embeddingHandler \\\\\\ affective.core.CSVEmbeddingHandler -K $HOME/wekafiles/packages/AffectiveTweets/resources/w2v.twitter.edinburgh.100d.csv.gz -sep \\\\\\\\\\\\\\ \\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\  -I last\\\\\\  -K 15 -stemmer weka.core.stemmers.NullStemmer -stopwords-handler \\\\\\ weka.core.stopwords.Null \\\\\\  -I 1 -U -tokenizer \\\\\\ weka.core.tokenizers.TweetNLPTokenizer \\\\\\ \\  -F \\ weka.filters.unsupervised.attribute.Reorder -R 4-last,3\\  -W weka.classifiers.functions.LibLINEAR -- -S 1 -C 1.0 -E 0.001 -B 1.0 -L 0.1 -I 1000", 
            "title": "Tweet Classification from the CL"
        }, 
        {
            "location": "/examples/#feature-extraction-from-the-cl", 
            "text": "There is also possible to run filters in isolation and then convert the processed files into CSV files:   First run a filter:   java -Xmx4G -cp weka.jar weka.Run weka.filters.unsupervised.attribute.TweetToInputLexiconFeatureVector  -i  $HOME/wekafiles/packages/AffectiveTweets/data/sent140test.arff.gz -o proc_data.arff -lexicon_evaluator  affective.core.ArffLexiconEvaluator -lexiconFile /Users/admin/wekafiles/packages/AffectiveTweets/lexicons/arff_lexicons/NRC-AffectIntensity-Lexicon.arff -B NRC-Affect-Intensity -A 1 -lex-stemmer weka.core.stemmers.NullStemmer  -stemmer weka.core.stemmers.NullStemmer -stopwords-handler  weka.core.stopwords.Null   -I 1 -U -tokenizer  weka.core.tokenizers.TweetNLPTokenizer     Then, convert the new feature vector into a CSV file.   java -Xmx4G -cp weka.jar weka.core.converters.CSVSaver -i proc_data.arff -o proc_data.csv   More information about how to run filters from the command line on the test data can be found  here .", 
            "title": "Feature Extraction from the CL"
        }
    ]
}